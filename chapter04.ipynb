{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章  基于概率论的分类方法：朴素贝叶斯\n",
    "\n",
    "## 本章内容\n",
    "\n",
    "* 使用概率分布进行分类\n",
    "* 学习朴素贝叶斯分类器\n",
    "* 了解RSS源数据\n",
    "* 使用朴素贝叶斯来分析不同地区的态度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 朴素贝叶斯 概述\n",
    "\n",
    "**朴素贝叶斯** 是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。朴素贝叶斯是贝叶斯决策理论的一部分，因此在开始朴素贝叶斯之前，我们先了解一下贝叶斯决策理论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 朴素贝叶斯理论 & 条件概率\n",
    "\n",
    "### 4.2.1 贝叶斯理论\n",
    "\n",
    "假设我们有一个数据集，它有两类数据组成，如下图所示：\n",
    "\n",
    "![](images/sample_plot.png)\n",
    "\n",
    "我们用 $p1(x, y)$ 表示数据点 $(x, y)$属于类别 1 的概率，用 $p2(x, y)$ 表示数据点 $(x, y)$属于类别 2 的概率，那么对于一个新的数据点$(x, y)$，可以用如下的规则判断它的类别：\n",
    "\n",
    "* 如果 $p1(x, y) > p2(x, y)$，那么类别为 1；\n",
    "* 如果 $p2(x, y) > p1(x, y)$，那么类别为 2。\n",
    "\n",
    "也就是说我们会选择高概率对应的累类别，这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 条件概率\n",
    "\n",
    "假设一个罐子里装了7块石头，其中3块是白色的，4块是黑色的。如果从罐子里随机取出一块石头，那么取出的石头是白色的可能性是多少？由于取石头有7中可能，其中3中为白色，所以取出白色石头的概率为 3/7，同理取出黑色石头的概率是 4/7。\n",
    "\n",
    "假设我们使用 $P(white)$ 表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数得到。\n",
    "\n",
    "![](images/gray_black_stone.png)\n",
    "![](images/two_tone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果事先我们知道石头所在桶的信息会改变的结果的前提下，要计算 $P(white)$ 或者 $P(black)$。嘉定计算的是从 B 桶取到白色石头的概率，这个概率我们记作 $P(white|bucketB)$，读作“在已知石头出自 B 桶的条件下，取出白色石头的概率”，这就是**条件概率（conditional probablity）**。从前一个例子中我们很容易的， $P(white|bucketB) = 1/3，P(white|bucketA) = 1/2$。如何计算的呢？\n",
    "\n",
    "计算公式如下：\n",
    "\n",
    "$$P(white|bucketB) = P(white\\quad and\\quad bucketB|bucketB) / P(bucketB)$$\n",
    "\n",
    "首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 $ P(white and buckeB) = 1/7$，其次，由于 B 桶中有3块石头，而总是为7，于是 $P(bucketB) = 3/7 $，于是有：\n",
    "\n",
    "$$P(white|bucketB) = P(white\\quad and\\quad bucketB|bucketB) / P(bucketB) = (1/7)/(3/7) = 1/3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一种计算条件概率的方法称为**贝叶斯准则**，贝叶斯准则告诉我们如何交换条件概率中的条件和结果，即如何结果已知 $P(x|c)$，要求 $P(c|x)$，那么可以使用如下公式计算：\n",
    "\n",
    "$$\\mathscr P(c|x) = \\frac {\\mathscr P(x|c) \\mathscr P(c)}{\\mathscr P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 使用条件概率进行分类\n",
    "\n",
    "上面我们提到贝叶斯决策理论中，要求计算两个概率 $p1(x, y)$,$p2(x, y)$:\n",
    "\n",
    "* 如果 $p1(x, y) > p2(x, y)$，那么类别为 1；\n",
    "* 如果 $p2(x, y) > p1(x, y)$，那么类别为 2。\n",
    "\n",
    "但是这并不是贝叶斯决策理论的所有内容，这里使用$p1(), p2()$仅仅是为了简化描述，而实际上需要计算和比较的是 $p(c1|x, y)$和$p(c2|x, y)$，代表的意义是：在给定某个数据点(x、y)，那么该数据点来自类别c1的概率是多少？数据点来自c2的概率是多少？注意：这里所说的$p(c1|x, y)$和$p(x, y|c1)$并不一样，不过可以使用贝叶斯准则来交换概率中的条件和结果，具体的，应用贝叶斯准则得到：\n",
    "\n",
    "$$p(c_i|x,y) = \\frac {p(x,y|c_i) p(c_i)}{p(x,y)}$$\n",
    "\n",
    "使用上面的定义，可以定位贝叶斯分类准则为：\n",
    "\n",
    "* 如果 $p(c_1|x, y) > p(c_2|x, y)$，那么类别为 $c_1$；\n",
    "* 如果 $p(c_2|x, y) > p(c_1|x, y)$，那么类别为 $c_2$。\n",
    "\n",
    "使用贝叶斯准则，可以通过已知的三个概率来计算未知的概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 朴素贝叶斯 应用场景\n",
    "\n",
    "\n",
    "在文档分类中，整个文档（例如一封邮件）是一个实例，而邮件中某些元素则构成了特征。我们观察文档中出现的每个词，并把词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。\n",
    "\n",
    "朴素贝叶斯是贝叶斯分类器的一个扩展，是用于文档分类常用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 朴素贝叶斯 工作原理\n",
    "\n",
    "* 提取所有文档中的词条并进行去重\n",
    "* 获取文档的所有类别\n",
    "* 计算每个类别中的文档数目\n",
    "* 对每篇训练文档：\n",
    "    * 对每个类别：\n",
    "        * 如果词条出现在文档中，则增加该词条的计数值（for循环或者矩阵相加）\n",
    "        * 增加所有词条的计数值（该类别下词条总数）\n",
    "* 对每个类别：\n",
    "    * 对每个词条：\n",
    "        * 将该词条的数据除以总的词条收，得到条件概率（P(词条|类别)）\n",
    "* 返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 朴素贝叶斯 开发流程\n",
    "\n",
    "* 收集数据：可以是任何方法\n",
    "* 准备数据：需要数值型或者布尔型数据\n",
    "* 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n",
    "* 训练算法：计算不同的独立特征的条件概率\n",
    "* 测试算法：计算错误率\n",
    "* 使用算法：常见的使用场景是文档分类，不过可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 朴素贝叶斯 算法特点\n",
    "\n",
    "* **优点**：在数据较少的情况下，朴素贝叶斯依然有效，并可以处理多分类问题\n",
    "* **缺点**：对于输入数据的准备方式较为敏感\n",
    "* **适用的数据类型**： 标称型数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 朴素贝叶斯 实战案例\n",
    "\n",
    "**案例1：屏蔽社区留言板的侮辱性言论**\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "构建一个快速过滤器来屏蔽社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此类问题建立两个类别：**侮辱性和非侮辱性，使用 1 和 0 分别表示**\n",
    "\n",
    "* **收集数据**：可以是任何方法\n",
    "\n",
    "这里的数据收集，假设我们收集了如下的词表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    创建数据集\n",
    "    :return: 单词列表postingList, 所属类别classVec\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], \n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 1 代表侮辱性, 0 代表正常\n",
    "    return postingList, classVec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postingList 中含有 6 篇文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **准备数据**：从文本中构建词向量\n",
    "\n",
    "构建词向量就是创建一个包含在所有文档中出现的不重复的词的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 原始数据集\n",
    "    :return: 所有不含有重复单词的集合\n",
    "    \"\"\"\n",
    "    vocabSet = set([]) # 由于需要不重复的集合，因此这里使用`set`\n",
    "    for document in dataSet:\n",
    "        # 这里使用按位或操作符 | ，求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取了单词集合，也可以叫词汇表之后，我们需要计算词汇表的词汇向量，向量的每一个元素为 1 或者 0，分别代表词汇表中的单词在输入的文档中是否出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看某个单词是否在词汇表中出现，如果出现则设置该词为1\n",
    "    :param vocabList: 所有单词的集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表 [0， 1， 0， 1，1...]，其中 1与0 表示词汇表中的单词是否出现\n",
    "    \"\"\"\n",
    "    # 首先创建一个和词汇表等长的向量，并将元素均置为0\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    # 遍历数据集中的所有单词，如果出现词汇表中的单词，则将输出的文档向量中对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print('the word: %s is not in my Vocabulary!') % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **分析数据**：检查词条确保解析的正确性\n",
    "\n",
    "这一步在这里主要是检查上述的函数是否执行正确，检查输出的词汇表是否是否没有重复单词，需要的话，可以对其进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "listOPosts, listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['him',\n",
       " 'garbage',\n",
       " 'ate',\n",
       " 'cute',\n",
       " 'to',\n",
       " 'quit',\n",
       " 'licks',\n",
       " 'dog',\n",
       " 'please',\n",
       " 'posting',\n",
       " 'problems',\n",
       " 'so',\n",
       " 'maybe',\n",
       " 'stupid',\n",
       " 'take',\n",
       " 'stop',\n",
       " 'park',\n",
       " 'my',\n",
       " 'how',\n",
       " 'buying',\n",
       " 'food',\n",
       " 'help',\n",
       " 'dalmation',\n",
       " 'not',\n",
       " 'I',\n",
       " 'love',\n",
       " 'flea',\n",
       " 'worthless',\n",
       " 'steak',\n",
       " 'is',\n",
       " 'has',\n",
       " 'mr']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词汇表获得之后，我们检查词向量函数的有效性。例如，词汇表中索引为 5 的元素是什么单词？通过查看词汇表知道应该是 so。并且该单词应该是在第三篇文档中出现的，其他的文档中没有，我们调用函数看看情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **训练算法**：从词向量计算概率\n",
    "\n",
    "通过上面的示例，我们已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。\n",
    "\n",
    "接下来重写一下贝叶斯准则，将之前的x，y替换为 $\\mathbf w$， $\\mathbf w$表示一个向量，即它是由多个值组成。在这个例子中，数值个数与词汇表中词的个数相同。\n",
    "\n",
    "$$p(c_i| \\mathbf w) = \\frac {p(\\mathbf w|c_i)p(c_i)}{p(\\mathbf w)}$$\n",
    "\n",
    "下面使用上面的公式，对每个类进行计算，得到概率，然后比较两个概率的大小。那么如何计算呢？\n",
    "\n",
    "* 通过类别 i 中文档的数目除以在那个的文档数目，得到概率 $p(c_i)$;\n",
    "* 使用贝叶斯假设，计算$p(\\mathbf w|c_i)$。\n",
    "    * 如果将 $\\mathbf w$ 展开为一个独立特征，那么就可以将上述概率写作 $p(w_0,w_1,w_2...w_n|c_i)$\n",
    "    * 假设所有词都是独立的（也称作条件独立性假设，例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），那就意味可以使用$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_n|c_i)$来计算上述概率，也简化了计算过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def _trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯训练函数（原始版本）\n",
    "    :param trainMatrix: 文档当次矩阵，也就是上面所将的词向量。[[1,1,0,1,0,0],[0,1,0,1,0,1]...]\n",
    "    :param trainCategory: 文档对应的类别[0,1,1,0...]，长度等于单词矩阵数，其中 1 代表对应文档是侮辱性文档，0 代表不是\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 获取文档数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 获取单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 计算侮辱性文档出现的概率，即trainCategory中所有的分类为 1 的个数，\n",
    "    # 代表就是多少个侮辱性文档，与文件总数相除，得到侮辱性文档出现的概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词的出现次数列表\n",
    "    p0Num = zeros(numWords)\n",
    "    p1Num = zeros(numWords)\n",
    "    \n",
    "    # 整个数据集单词出现的总数\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 判断是否是侮辱性文档\n",
    "        if trainCategory[i] == 1:\n",
    "            # 如果是侮辱性文档，对侮辱性文档的向量进行加和\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对向量中的所有元素进行求和，也就是计算所有侮辱性文档中出现单词的总数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表\n",
    "    # 即 在1类别下，每个单词出现的概率\n",
    "    p1Vect = p1Num / p1Denom# [1,2,3,5]/90->[1/90,...]\n",
    "    # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    # 即 在0类别下，每个单词出现的概率\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb = _trainNB0(trainMat, listClasses)\n",
    "# 任意文档属于侮辱性文档的概率\n",
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08333333,  0.        ,  0.04166667,  0.04166667,  0.04166667,\n",
       "        0.        ,  0.04166667,  0.04166667,  0.04166667,  0.        ,\n",
       "        0.04166667,  0.04166667,  0.        ,  0.        ,  0.        ,\n",
       "        0.04166667,  0.        ,  0.125     ,  0.04166667,  0.        ,\n",
       "        0.        ,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "        0.04166667,  0.04166667,  0.        ,  0.04166667,  0.04166667,\n",
       "        0.04166667,  0.04166667])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇表中每个词属于正常言论的概率\n",
    "p0V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05263158,  0.05263158,  0.        ,  0.        ,  0.05263158,\n",
       "        0.05263158,  0.        ,  0.10526316,  0.        ,  0.05263158,\n",
       "        0.        ,  0.        ,  0.05263158,  0.15789474,  0.05263158,\n",
       "        0.05263158,  0.05263158,  0.        ,  0.        ,  0.05263158,\n",
       "        0.05263158,  0.        ,  0.        ,  0.05263158,  0.        ,\n",
       "        0.        ,  0.        ,  0.10526316,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇表中每个词属于侮辱性言论的概率\n",
    "p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **测试算法**：根据实际情况修改分类器\n",
    "\n",
    "但是该算法实现，在实际应用用还有一些缺陷。例如在计算$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_n|c_i)$的时候，其中任意一个概率为0，则直接导致结果为0，为了降低这种影响，可以将所有词的出现初始化为1，并将分母初始化为2.\n",
    "\n",
    "```\n",
    "   # 构造单词的出现次数列表\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    \n",
    "    # 整个数据集单词出现的总数\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "```\n",
    "\n",
    "还有一个问题就是**下溢出**，这是由于太多很小的数相乘造成的。当计算$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_n|c_i)$的时候，由于大部分的计算因子都很小，所以会有下溢出的情况发生，造成结果不正确，例如下面计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2320129599999984e-24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re = 0.00000158 * 0.00000158 * 0.00000158 * 0.00000158 \n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decimal('0.000000')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "round(Decimal(re), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决这个问题，一般的解决办法是**对乘积取自然对数**。在代数中有 $ln(a * b) = ln(a) + ln(b)$, 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。\n",
    "\n",
    "下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。\n",
    "\n",
    "![](images/ln.png)\n",
    "\n",
    "\n",
    "了解了这些后，我们改造上述贝叶斯训练函数，得到新的训练函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯训练函数（优化版本）\n",
    "    :param trainMatrix: 文档当次矩阵，也就是上面所将的词向量。[[1,1,0,1,0,0],[0,1,0,1,0,1]...]\n",
    "    :param trainCategory: 文档对应的类别[0,1,1,0...]，长度等于单词矩阵数，其中 1 代表对应文档是侮辱性文档，0 代表不是\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 获取文档数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 获取单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 计算侮辱性文档出现的概率，即trainCategory中所有的分类为 1 的个数，\n",
    "    # 代表就是多少个侮辱性文档，与文件总数相除，得到侮辱性文档出现的概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词的出现次数列表(#[0,0......]->[1,1,1,1,1.....])\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    \n",
    "    # 整个数据集单词出现的总数,2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 判断是否是侮辱性文档\n",
    "        if trainCategory[i] == 1:\n",
    "            # 如果是侮辱性文档，对侮辱性文档的向量进行加和\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对向量中的所有元素进行求和，也就是计算所有侮辱性文档中出现单词的总数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "# 任意文档属于侮辱性文档的概率\n",
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.15948425, -3.25809654, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -3.25809654, -1.87180218, -2.56494936, -3.25809654,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -2.56494936])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.35137526, -2.35137526, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -2.35137526, -3.04452244, -1.94591015, -3.04452244, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -1.65822808, -2.35137526,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -2.35137526, -3.04452244, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -1.94591015, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **使用算法**：对社区留言板言论进行分类\n",
    "\n",
    "有了上述相关特征提取，训练方法定义后，我们队文本内容进行分类，分类器代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param pClass1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，\n",
    "    # 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1) # P(w|c1) * P(c1) ，即贝叶斯准则的分子\n",
    "    print('p1:', p1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) # P(w|c0) * P(c0) ，即贝叶斯准则的分子·\n",
    "    print('p0:', p0)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了分类函数之后，我们对整个训练和分类过程进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    # 2. 创建单词集合\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # 4. 训练数据\n",
    "    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    # 5. 测试数据\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: -9.82671449373\n",
      "p0: -7.69484807238\n",
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "p1: -4.70275051433\n",
      "p0: -7.2093402566\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "截止目前，我们将每个词是否出现作为一个特征，这种方式训练的模型可以称为**词集模型（set-of-words model）**，但是往往一个词在一个文档中不仅仅只会出现一次，每次的出现可能包含着不同的含义。在文本分类的案例中，经常会使用**词袋模型（bag-of-words model）**，在这个模型中，每个词可以出现多次，但是词集中只能出现一次，为了适应词袋模型，需要对函数`setOfWords2Vec()`进行重新修改，假设修改后的函数名为`bagOfWordsVec()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**案例2：使用朴素贝叶斯过滤垃圾邮件**\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。\n",
    "\n",
    "**开发流程**\n",
    "\n",
    "* **收集数据**: 提供文本文件\n",
    "* **准备数据**: 将文本文件解析成词条向量\n",
    "* **分析数据**: 检查词条确保解析的正确性\n",
    "* **训练算法**: 使用我们之前建立的 trainNB() 函数\n",
    "* **测试算法**: 使用朴素贝叶斯进行交叉验证\n",
    "* **使用算法**: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**收集数据**：提供文本文件\n",
    "\n",
    "假设我们有如下内容的一封邮电：\n",
    "\n",
    "> Hi Peter,\n",
    "\n",
    "> With Jose out of town, do you want to\n",
    "meet once in a while to keep things\n",
    "going and do some interesting stuff?\n",
    "\n",
    "> Let me know\n",
    "> Eugene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备数据**：将文本文件解析成词条向量\n",
    "\n",
    "为了得到词向量，我们可以将文件内容看做是一个字符串，然后对这个字符串进行切分，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Peter,',\n",
       " 'With',\n",
       " 'Jose',\n",
       " 'out',\n",
       " 'of',\n",
       " 'town,',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'once',\n",
       " 'in',\n",
       " 'a',\n",
       " 'while',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'things',\n",
       " 'going',\n",
       " 'and',\n",
       " 'do',\n",
       " 'some',\n",
       " 'interesting',\n",
       " 'stuff?',\n",
       " 'Let',\n",
       " 'me',\n",
       " 'know',\n",
       " 'Eugene']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent = \"Hi Peter,\\nWith Jose out of town, do you want to meet once in a while to keep things going and do some interesting stuff?\\nLet me know\\nEugene\"\n",
    "mySent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到切分的效果不错，但是有一个问题，内容中的标点符号也被作为词条了。而已使用正则表达式来切分，其中分隔符是除了单词、数字外的任意字符串："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Robin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Peter',\n",
       " 'With',\n",
       " 'Jose',\n",
       " 'out',\n",
       " 'of',\n",
       " 'town',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'once',\n",
       " 'in',\n",
       " 'a',\n",
       " 'while',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'things',\n",
       " 'going',\n",
       " 'and',\n",
       " 'do',\n",
       " 'some',\n",
       " 'interesting',\n",
       " 'stuff',\n",
       " 'Let',\n",
       " 'me',\n",
       " 'know',\n",
       " 'Eugene']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "mySent = \"Hi Peter,\\nWith Jose out of town, do you want to meet once in a while to keep things going and do some interesting stuff?\\nLet me know\\nEugene\"\n",
    "\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分好之后，为了能够让每个词条的格式保持统一，我么再次进行格式化，统一每个词条为小写，并且为了确保词条的有效性，我们只获取词条长度大于0的项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'peter',\n",
       " 'with',\n",
       " 'jose',\n",
       " 'out',\n",
       " 'of',\n",
       " 'town',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'once',\n",
       " 'in',\n",
       " 'a',\n",
       " 'while',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'things',\n",
       " 'going',\n",
       " 'and',\n",
       " 'do',\n",
       " 'some',\n",
       " 'interesting',\n",
       " 'stuff',\n",
       " 'let',\n",
       " 'me',\n",
       " 'know',\n",
       " 'eugene']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    \"\"\"\n",
    "    接收一个大字符串并将其解析为字符串列表\n",
    "    :param bigString: 大字符串\n",
    "    :return list: 去掉长度少于2的字符，并将每个词条转换为小写，组成列表返回\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # 使用正则表达式切分字符串，其中分隔符是除了单词、数字外的任意字符串\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分析数据**：检查词条确保解析的正确性\n",
    "\n",
    "此步骤已经包含在了上一步中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练算法**：这里直接使用上一个例子中的trainNB0()函数即可\n",
    "\n",
    "**测试算法**：使用朴素贝叶斯进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    # 文件信息矩阵\n",
    "    docList = []\n",
    "    # 分类矩阵\n",
    "    classList = []\n",
    "    # 全部文本信息矩阵\n",
    "    fullText = []\n",
    "    # 循环25次读取两个文件夹中50个邮件文本\n",
    "    for i in range(1, 26):\n",
    "        # 处理1类文本信息\n",
    "        wordList = textParse(open('resource/NaiveBayes/email/spam/%d.txt' % i, encoding=\"ascii\", errors='replace').read())\n",
    "        # 衔接到文件信息矩阵\n",
    "        docList.append(wordList)\n",
    "        # 拓展到全部文本信息矩阵\n",
    "        fullText.extend(wordList)\n",
    "        # 分类信息衔接1类标志\n",
    "        classList.append(1)\n",
    "        # 处理0类文本信息\n",
    "        wordList = textParse(open('resource/NaiveBayes/email/ham/%d.txt' % i, encoding=\"ascii\", errors='replace').read())\n",
    "        # 衔接到文件信息矩阵\n",
    "        docList.append(wordList)\n",
    "        # 拓展到全部文本信息矩阵\n",
    "        fullText.extend(wordList)\n",
    "        # 分类信息衔接0类标志\n",
    "        classList.append(0)\n",
    "    # 依据收集的文本信息，创建词汇集合\n",
    "    vocabList = createVocabList(docList)\n",
    "    # 初始化训练集合索引记录1-50\n",
    "    trainingSet = list(range(50))\n",
    "    # 初始测试矩阵\n",
    "    testSet = []\n",
    "    # 随机选取十个信息数据作为测试集\n",
    "    for i in range(10):\n",
    "        # 1-50随机选取一个数字\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        # 将选取到的衔接到测试集索引区\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        # 删除训练集对应索引\n",
    "        del (trainingSet[randIndex])\n",
    "    # 初始化训练矩阵\n",
    "    trainMat = []\n",
    "    # 初始化分类矩阵\n",
    "    trainClasses = []\n",
    "    # 遍历训练集索引，此处有40个\n",
    "    for docIndex in trainingSet:\n",
    "        # 构造训练集矩阵，依据词袋函数\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        # 构造训练集对应的分类矩阵\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 计算0分类语义概率，1分类语义概率，1分类概率\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    # 初始化预测错误次数\n",
    "    errorCount = 0\n",
    "    # 遍历测试集，此处有10个\n",
    "    for docIndex in testSet:\n",
    "        # 构造词袋向量\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        # 判定预测与实际值异同\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            # 不一样累加1\n",
    "            errorCount += 1\n",
    "            print(\"classification error\", docList[docIndex])\n",
    "    # 计算并打印预测错误率\n",
    "    print('the error rate is: ', float(errorCount) / len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Robin/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: -493.013482125\n",
      "p0: -459.285684265\n",
      "p1: -205.740024878\n",
      "p0: -196.452459189\n",
      "classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "p1: -239.754878659\n",
      "p0: -291.236246541\n",
      "p1: -165.051566721\n",
      "p0: -156.710380087\n",
      "p1: -105.538903564\n",
      "p0: -97.92365179\n",
      "p1: -115.413239848\n",
      "p0: -107.531167723\n",
      "p1: -166.760930407\n",
      "p0: -189.392894679\n",
      "p1: -328.356670247\n",
      "p0: -301.199736477\n",
      "p1: -98.0797093342\n",
      "p0: -119.332212781\n",
      "p1: -94.5422384642\n",
      "p0: -86.7620803532\n",
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
