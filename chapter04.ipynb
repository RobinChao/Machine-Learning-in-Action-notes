{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章  基于概率论的分类方法：朴素贝叶斯\n",
    "\n",
    "## 本章内容\n",
    "\n",
    "* 使用概率分布进行分类\n",
    "* 学习朴素贝叶斯分类器\n",
    "* 了解RSS源数据\n",
    "* 使用朴素贝叶斯来分析不同地区的态度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 朴素贝叶斯 概述\n",
    "\n",
    "**朴素贝叶斯** 是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。朴素贝叶斯是贝叶斯决策理论的一部分，因此在开始朴素贝叶斯之前，我们先了解一下贝叶斯决策理论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 朴素贝叶斯理论 & 条件概率\n",
    "\n",
    "### 4.2.1 贝叶斯理论\n",
    "\n",
    "假设我们有一个数据集，它有两类数据组成，如下图所示：\n",
    "\n",
    "![](images/sample_plot.png)\n",
    "\n",
    "我们用 $p1(x, y)$ 表示数据点 $(x, y)$属于类别 1 的概率，用 $p2(x, y)$ 表示数据点 $(x, y)$属于类别 2 的概率，那么对于一个新的数据点$(x, y)$，可以用如下的规则判断它的类别：\n",
    "\n",
    "* 如果 $p1(x, y) > p2(x, y)$，那么类别为 1；\n",
    "* 如果 $p2(x, y) > p1(x, y)$，那么类别为 2。\n",
    "\n",
    "也就是说我们会选择高概率对应的累类别，这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 条件概率\n",
    "\n",
    "假设一个罐子里装了7块石头，其中3块是白色的，4块是黑色的。如果从罐子里随机取出一块石头，那么取出的石头是白色的可能性是多少？由于取石头有7中可能，其中3中为白色，所以取出白色石头的概率为 3/7，同理取出黑色石头的概率是 4/7。\n",
    "\n",
    "假设我们使用 $P(white)$ 表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数得到。\n",
    "\n",
    "![](images/gray_black_stone.png)\n",
    "![](images/two_tone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果事先我们知道石头所在桶的信息会改变的结果的前提下，要计算 $P(white)$ 或者 $P(black)$。嘉定计算的是从 B 桶取到白色石头的概率，这个概率我们记作 $P(white|bucketB)$，读作“在已知石头出自 B 桶的条件下，取出白色石头的概率”，这就是**条件概率（conditional probablity）**。从前一个例子中我们很容易的， $P(white|bucketB) = 1/3，P(white|bucketA) = 1/2$。如何计算的呢？\n",
    "\n",
    "计算公式如下：\n",
    "\n",
    "$$P(white|bucketB) = P(white\\quad and\\quad bucketB|bucketB) / P(bucketB)$$\n",
    "\n",
    "首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 $ P(white and buckeB) = 1/7$，其次，由于 B 桶中有3块石头，而总是为7，于是 $P(bucketB) = 3/7 $，于是有：\n",
    "\n",
    "$$P(white|bucketB) = P(white\\quad and\\quad bucketB|bucketB) / P(bucketB) = (1/7)/(3/7) = 1/3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一种计算条件概率的方法称为**贝叶斯准则**，贝叶斯准则告诉我们如何交换条件概率中的条件和结果，即如何结果已知 $P(x|c)$，要求 $P(c|x)$，那么可以使用如下公式计算：\n",
    "\n",
    "$$\\mathscr P(c|x) = \\frac {\\mathscr P(x|c) \\mathscr P(c)}{\\mathscr P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 使用条件概率进行分类\n",
    "\n",
    "上面我们提到贝叶斯决策理论中，要求计算两个概率 $p1(x, y)$,$p2(x, y)$:\n",
    "\n",
    "* 如果 $p1(x, y) > p2(x, y)$，那么类别为 1；\n",
    "* 如果 $p2(x, y) > p1(x, y)$，那么类别为 2。\n",
    "\n",
    "但是这并不是贝叶斯决策理论的所有内容，这里使用$p1(), p2()$仅仅是为了简化描述，而实际上需要计算和比较的是 $p(c1|x, y)$和$p(c2|x, y)$，代表的意义是：在给定某个数据点(x、y)，那么该数据点来自类别c1的概率是多少？数据点来自c2的概率是多少？注意：这里所说的$p(c1|x, y)$和$p(x, y|c1)$并不一样，不过可以使用贝叶斯准则来交换概率中的条件和结果，具体的，应用贝叶斯准则得到：\n",
    "\n",
    "$$p(c_i|x,y) = \\frac {p(x,y|c_i) p(c_i)}{p(x,y)}$$\n",
    "\n",
    "使用上面的定义，可以定位贝叶斯分类准则为：\n",
    "\n",
    "* 如果 $p(c_1|x, y) > p(c_2|x, y)$，那么类别为 $c_1$；\n",
    "* 如果 $p(c_2|x, y) > p(c_1|x, y)$，那么类别为 $c_2$。\n",
    "\n",
    "使用贝叶斯准则，可以通过已知的三个概率来计算未知的概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 朴素贝叶斯 应用场景\n",
    "\n",
    "\n",
    "在文档分类中，整个文档（例如一封邮件）是一个实例，而邮件中某些元素则构成了特征。我们观察文档中出现的每个词，并把词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。\n",
    "\n",
    "朴素贝叶斯是贝叶斯分类器的一个扩展，是用于文档分类常用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 朴素贝叶斯 工作原理\n",
    "\n",
    "* 提取所有文档中的词条并进行去重\n",
    "* 获取文档的所有类别\n",
    "* 计算每个类别中的文档数目\n",
    "* 对每篇训练文档：\n",
    "    * 对每个类别：\n",
    "        * 如果词条出现在文档中，则增加该词条的计数值（for循环或者矩阵相加）\n",
    "        * 增加所有词条的计数值（该类别下词条总数）\n",
    "* 对每个类别：\n",
    "    * 对每个词条：\n",
    "        * 将该词条的数据除以总的词条收，得到条件概率（P(词条|类别)）\n",
    "* 返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 朴素贝叶斯 开发流程\n",
    "\n",
    "* 收集数据：可以是任何方法\n",
    "* 准备数据：需要数值型或者布尔型数据\n",
    "* 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n",
    "* 训练算法：计算不同的独立特征的条件概率\n",
    "* 测试算法：计算错误率\n",
    "* 使用算法：常见的使用场景是文档分类，不过可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 朴素贝叶斯 算法特点\n",
    "\n",
    "* **优点**：在数据较少的情况下，朴素贝叶斯依然有效，并可以处理多分类问题\n",
    "* **缺点**：对于输入数据的准备方式较为敏感\n",
    "* **适用的数据类型**： 标称型数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 朴素贝叶斯 实战案例\n",
    "\n",
    "**案例1：屏蔽社区留言板的侮辱性言论**\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "构建一个快速过滤器来屏蔽社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此类问题建立两个类别：**侮辱性和非侮辱性，使用 1 和 0 分别表示**\n",
    "\n",
    "* **收集数据**：可以是任何方法\n",
    "\n",
    "这里的数据收集，假设我们收集了如下的词表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    创建数据集\n",
    "    :return: 单词列表postingList, 所属类别classVec\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], \n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 1 代表侮辱性, 0 代表正常\n",
    "    return postingList, classVec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postingList 中含有 6 篇文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **准备数据**：从文本中构建词向量\n",
    "\n",
    "构建词向量就是创建一个包含在所有文档中出现的不重复的词的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 原始数据集\n",
    "    :return: 所有不含有重复单词的集合\n",
    "    \"\"\"\n",
    "    vocabSet = set([]) # 由于需要不重复的集合，因此这里使用`set`\n",
    "    for document in dataSet:\n",
    "        # 这里使用按位或操作符 | ，求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取了单词集合，也可以叫词汇表之后，我们需要计算词汇表的词汇向量，向量的每一个元素为 1 或者 0，分别代表词汇表中的单词在输入的文档中是否出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看某个单词是否在词汇表中出现，如果出现则设置该词为1\n",
    "    :param vocabList: 所有单词的集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表 [0， 1， 0， 1，1...]，其中 1与0 表示词汇表中的单词是否出现\n",
    "    \"\"\"\n",
    "    # 首先创建一个和词汇表等长的向量，并将元素均置为0\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    # 遍历数据集中的所有单词，如果出现词汇表中的单词，则将输出的文档向量中对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print('the word: %s is not in my Vocabulary!') % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **分析数据**：检查词条确保解析的正确性\n",
    "\n",
    "这一步在这里主要是检查上述的函数是否执行正确，检查输出的词汇表是否是否没有重复单词，需要的话，可以对其进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "listOPosts, listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'to',\n",
       " 'park',\n",
       " 'quit',\n",
       " 'please',\n",
       " 'so',\n",
       " 'dalmation',\n",
       " 'love',\n",
       " 'him',\n",
       " 'is',\n",
       " 'flea',\n",
       " 'problems',\n",
       " 'cute',\n",
       " 'worthless',\n",
       " 'not',\n",
       " 'take',\n",
       " 'buying',\n",
       " 'how',\n",
       " 'dog',\n",
       " 'mr',\n",
       " 'food',\n",
       " 'my',\n",
       " 'help',\n",
       " 'stupid',\n",
       " 'maybe',\n",
       " 'I',\n",
       " 'stop',\n",
       " 'posting',\n",
       " 'steak',\n",
       " 'licks',\n",
       " 'garbage',\n",
       " 'has']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词汇表获得之后，我们检查词向量函数的有效性。例如，词汇表中索引为 5 的元素是什么单词？通过查看词汇表知道应该是 so。并且该单词应该是在第三篇文档中出现的，其他的文档中没有，我们调用函数看看情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **训练算法**：从词向量计算概率\n",
    "\n",
    "通过上面的示例，我们已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。\n",
    "\n",
    "接下来重写一下贝叶斯准则，将之前的x，y替换为 $\\mathbf w$， $\\mathbf w$表示一个向量，即它是由多个值组成。在这个例子中，数值个数与词汇表中词的个数相同。\n",
    "\n",
    "$$p(c_i| \\mathbf w) = \\frac {p(\\mathbf w|c_i)p(c_i)}{p(\\mathbf w)}$$\n",
    "\n",
    "下面使用上面的公式，对每个类进行计算，得到概率，然后比较两个概率的大小。那么如何计算呢？\n",
    "\n",
    "* 通过类别 i 中文档的数目除以在那个的文档数目，得到概率 $p(c_i)$;\n",
    "* 使用贝叶斯假设，计算$p(\\mathbf w|c_i)$。\n",
    "    * 如果将 $\\mathbf w$ 展开为一个独立特征，那么就可以将上述概率写作 $p(w_0,w_1,w_2...w_n|c_i)$\n",
    "    * 假设所有词都是独立的（也称作条件独立性假设，例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），那就意味可以使用$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_n|c_i)$来计算上述概率，也简化了计算过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯训练函数（原始版本）\n",
    "    :param trainMatrix: 文档当次矩阵，也就是上面所将的词向量。[[1,1,0,1,0,0],[0,1,0,1,0,1]...]\n",
    "    :param trainCategory: 文档对应的类别[0,1,1,0...]，长度等于单词矩阵数，其中 1 代表对应文档是侮辱性文档，0 代表不是\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 获取文档数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 获取单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 计算侮辱性文档出现的概率，即trainCategory中所有的分类为 1 的个数，\n",
    "    # 代表就是多少个侮辱性文档，与文件总数相除，得到侮辱性文档出现的概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词的出现次数列表\n",
    "    p0Num = zeros(numWords)\n",
    "    p1Num = zeros(numWords)\n",
    "    \n",
    "    # 整个数据集单词出现的总数\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 判断是否是侮辱性文档\n",
    "        if trainCategory[i] == 1:\n",
    "            # 如果是侮辱性文档，对侮辱性文档的向量进行加和\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对向量中的所有元素进行求和，也就是计算所有侮辱性文档中出现单词的总数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表\n",
    "    # 即 在1类别下，每个单词出现的概率\n",
    "    p1Vect = p1Num / p1Denom# [1,2,3,5]/90->[1/90,...]\n",
    "    # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    # 即 在0类别下，每个单词出现的概率\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
