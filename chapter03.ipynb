{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章  决策树\n",
    "\n",
    "## 本章内容\n",
    "\n",
    "* 决策树简介\n",
    "* 在数据集中度量一致性\n",
    "* 使用递归构造决策树\n",
    "* 使用Matplotlib绘制树形图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 决策树 概述\n",
    "\n",
    "**决策树（Decision Tree）**算法是一种基本的分类和回归方法，是最经常使用的数据挖掘算法之一，但是这里我们先讨论用于**分类**的决策树。\n",
    "\n",
    "**决策树模型**：呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 **if-then**规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。\n",
    "\n",
    "**决策树**学习通常包含以下三个步骤：\n",
    "\n",
    "1. 特征选择\n",
    "2. 决策树生成\n",
    "3. 决策树修剪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 决策树 应用场景\n",
    "\n",
    "这里以一个经典的“二十个问题”的游戏描述开始：\n",
    "\n",
    "游戏规则：\n",
    "\n",
    "参与游戏的一方在脑海中构想一个事物，其他参与者向他提问，只允许提20个问题，问题的答案也只能用对或者错回答。问问题的人通过推断分解，逐步的缩小猜想事物的范围，最后得到游戏的答案 --- 是一个什么样的场景。\n",
    "\n",
    "\n",
    "另一个比较能说明问题的例子是邮件分类系统，大致工作流程如下：\n",
    "\n",
    "![](images/decision_tree_email.jpg)\n",
    "\n",
    "首先检测发送邮件的域名地址，如果地址为 myEmployer.com，则将其放在分类 “无聊时需要阅读的邮件”中。如果邮件不是来自这个域名，则检测邮件内容是否包含单词“曲棍球”，如果包含则将邮件归类到“需要及时处理的朋友邮件”，否则归类到“无需阅读的垃圾邮件”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 决策树 定义\n",
    "\n",
    "**分类决策树模型**是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。节点有两种类型：\n",
    "\n",
    "* 内部节点（internal node）：表示一个特征或属性\n",
    "* 叶节点（leaf node）：表示一个类\n",
    "\n",
    "用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点，此时，每个子节点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶节点，最后将实例分配到到叶节点的类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 决策树 工作原理\n",
    "\n",
    "**信息熵 & 信息增益**\n",
    "\n",
    "**熵（entropy）**：指的是体系的混乱程度，在不同的学科中也有引申出的更为具体的定义，是领域十分重要的参量。(后面详解)\n",
    "\n",
    "**信息熵（香农熵）**：是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值最低，相反，熵值很高。(后面详解)\n",
    "\n",
    "**信息增益**：在划分数据集前后信息发生的变化成为信息增益。(后面详解)\n",
    "\n",
    "**决策树构造**（伪代码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*createBranch()*\n",
    "```\n",
    "检测数据集中每个子项是否属于同一分类：\n",
    "    If so return 类标签;\n",
    "    Else\n",
    "        寻找划分数据集的最好特征\n",
    "        划分数据集\n",
    "        创建分支点\n",
    "            for 每个划分的子集\n",
    "                调用构建决策树函数，并增加返回结果到分支节点中\n",
    "        return 分直节点\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 决策树 工作流程\n",
    "\n",
    "1. 收集数据：可以使用任何方法\n",
    "2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化\n",
    "3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图像是否符合预期\n",
    "4. 训练算法：构造树的数据结构\n",
    "5. 测试算法：使用经验树计算错误率\n",
    "6. 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 决策树 特点\n",
    "\n",
    "* **优点**：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据；\n",
    "* **缺点**：可能会产生过度匹配的问题\n",
    "* **使用数据类型**：数值型和标称型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 决策树 项目实践\n",
    "\n",
    "### 实践1： 判定是否是鱼类\n",
    "\n",
    "#### 项目概述\n",
    "\n",
    "根据以下两个特征，判断该动物是否为鱼类。\n",
    "\n",
    "特征：\n",
    "\n",
    "1. 不浮出水面是否可以生存\n",
    "2. 是否有脚蹼\n",
    "\n",
    "#### 开发流程\n",
    "\n",
    "1. 收集数据：可以使用任何方法\n",
    "2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化\n",
    "3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期\n",
    "4. 训练算法：构造树的数据结构\n",
    "5. 测试算法：使用决策树执行分类\n",
    "6. 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义\n",
    "\n",
    "**1. 收集数据**：可以使用任何方法\n",
    "\n",
    "假设我们得到的数据集如下：\n",
    "\n",
    "![](images/fish_or_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 准备数据**：树构造算法只适用于标称型数据，因此数值型数据必须离散化\n",
    "\n",
    "现在我们想要决定依据第一个特征还是第二个特征划分数据，但是在这之前，我们必须采用量化的方法判断如何划分数据。\n",
    "\n",
    "我们构建自己的 *createDataSet()* 方法，将标称型的数据离散化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']],\n",
       " ['no surfacing', 'flippers'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建数据样本集（数据变化）\n",
    "def createDataSet():\n",
    "    dataSet = [\n",
    "        [1, 1, 'yes'],\n",
    "        [1, 1, 'yes'],\n",
    "        [1, 0, 'no'],\n",
    "        [0, 1, 'no'],\n",
    "        [0, 1, 'no']\n",
    "    ]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels\n",
    "\n",
    "myData, labels = createDataSet()\n",
    "myData, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 分析数据**：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期\n",
    "\n",
    "\n",
    "**3.1 信息增益（information gain）**\n",
    "\n",
    "划分数据集的大原则是：**将无序的数据变得更加有序。**\n",
    "组织杂乱无章数据的一种方法：**使用信息论度量信息，信息论是量化处理信息的分支科学。**\n",
    "\n",
    "**信息增益 定义**\n",
    "\n",
    "在划分数据之前后，信息发生的变化成为信息增益。\n",
    "\n",
    "**如何计算信息增益**\n",
    "\n",
    "**香农熵（简称熵）**：是集合信息的度量方式，定义为信息的期望值。\n",
    "\n",
    "**信息 定义**\n",
    "\n",
    "如果待分类的事物可能被划分在多个分类中，则符号$x_i$事物的信息定义为：\n",
    "\n",
    "$$l(x_i) = -\\log_2{p(x_i)}$$\n",
    "\n",
    "其中 $p(x_i)$ 是选择该分类的概率。\n",
    "\n",
    "但是为了计算熵，需要计算所有类别所有可能值包含的信息期望值，通过如下公式得到：\n",
    "\n",
    "$$H = -\\sum_{i=1}^{n}p(x_i)\\log_2{p(x_i)}$$\n",
    "\n",
    "其中$n$为分类的数目。\n",
    "\n",
    "了解了**香农熵**计算方式之后，我们来实现香农熵的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算给定数据集的香农熵的函数\n",
    "\n",
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    # 数据样本中的实例总数\n",
    "    numEntries = len(dataSet)\n",
    "    # 分类标签出现次数\n",
    "    labelCounts = {}\n",
    "    # 遍历数据集中每个特征向量\n",
    "    for featVec in dataSet:\n",
    "        # 获取分类标签\n",
    "        currentLabel = featVec[-1]\n",
    "        # 为所有可能的分类创建存储字典，如果当前键值不存在，则加入。每个键值记录了当前类别出现的次数\n",
    "        # key: 分类标签  value：出现的次数\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "        \n",
    "    # 对比标签的占比，求出标签的香农熵\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        # 使用所有分类标签的发生频率计算类别出现的概率\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        # 计算香农熵\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "    return shannonEnt\n",
    "\n",
    "calcShannonEnt(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经知道，熵越高，混合的数据也越高，我们尝试改变数据集中第一个的分类为：mybe，观察熵的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 'mybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# myData[0][-1] = 'mybe'\n",
    "myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3709505944546687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEnt(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取熵之后，我们就可以按照获取最大信息增益的方法划分数据集了。\n",
    "\n",
    "> 还有一种度量数据无需程度的方法：**基尼不纯度(Gini impurity)**，简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他类别中的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 划分数据集**\n",
    "\n",
    "有了上面的知识储备后，我们就可以开始划分数据集并度量划分数据集的熵信息，判断是否正确划分数据集了。\n",
    "\n",
    "划分数据集的函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照给定的特征划分数据集\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    \"\"\"splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)\n",
    "        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中\n",
    "    Args:\n",
    "        dataSet 数据集                 待划分的数据集\n",
    "        axis 表示每一行的index列        划分数据集的特征\n",
    "        value 表示index列对应的value值   需要返回的特征的值。\n",
    "    Returns:\n",
    "        index列为value的数据集【该数据集需要排除index列】\n",
    "    \"\"\"\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        # 抽取axis列的值为value的数据\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            # extend 合并数据\n",
    "            reducedFeatVec.extend(featVec[axis + 1:])\n",
    "            # 追加到数据list中\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 假设我们以每行的第0列的值为1作为特征划分数据集\n",
    "splitDataSet(myData, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'no'], [1, 'no']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设我们以每行的第0列的值为0作为特征划分数据集\n",
    "splitDataSet(myData, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将遍历整个数据集，循环计算香农熵和划分数据集，找到最好的特征划分方式，熵计算将会告诉我们如何划分数据集是最好的数据组织方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infoGain =  0.4199730940219749 bestFeature =  0 0.9709505944546686 0.5509775004326937\n",
      "infoGain =  0.17095059445466854 bestFeature =  1 0.9709505944546686 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择最好的数据集划分方式\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataSet 数据集\n",
    "    Returns:\n",
    "        bestFeature 最优的特征列\n",
    "    \"\"\"\n",
    "    # 获取第一行有多少特征，由于最后一列是分类标签，因此数量-1\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    # 计算数据的原始信息熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    # 最优信息增益值、最优特征编号变量定义\n",
    "    bestInfoGain, BestFeature = 0.0, -1\n",
    "    # 遍历所有的特征\n",
    "    for i in range(numFeatures):\n",
    "        # 获取对应特征下的所有数据\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # 使用set对list数据进行排重\n",
    "        uniqueVals = set(featList)\n",
    "        # 创建一个临时的信息熵\n",
    "        newEntropy = 0.0\n",
    "        # 遍历每一列的value集合，计算该列的信息熵\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            # 计算概率\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            # 计算信息熵\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        # gain[信息增益]：划分数据集前后的信息变化，获取信息熵最大的值\n",
    "        # 信息增益是熵的减少或者数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        print('infoGain = ', infoGain, 'bestFeature = ', i, baseEntropy, newEntropy)\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "chooseBestFeatureToSplit(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，原始数据中，第0个特征是最好的用于划分数据集的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 训练算法**：构造树的数据结构\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
